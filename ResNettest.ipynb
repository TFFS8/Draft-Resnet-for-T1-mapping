{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Draft of a Resnet to input LR saturation time images and output HR T1 maps \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import add\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input, Dense, Add\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "(2, 336, 336, 9)\n",
      "validation\n",
      "(2, 336, 336, 3)\n",
      "(4, 336, 336, 9)\n",
      "training\n",
      "(4, 336, 336, 3)\n",
      "(4, 336, 336, 9)\n",
      "training\n",
      "(4, 336, 336, 3)\n",
      "1/2 [==============>...............] - ETA: 31s - loss: 864031187402752.0000(4, 336, 336, 9)\n",
      "training\n",
      "(4, 336, 336, 3)\n",
      "(4, 336, 336, 9)\n",
      "training\n",
      "(4, 336, 336, 3)\n",
      "(2, 336, 336, 9)\n",
      "validation\n",
      "(2, 336, 336, 3)\n",
      "2/2 [==============================] - 45s 22s/step - loss: 864031187402752.0000 - val_loss: 8670047006359552.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 8670047006359552.00000, saving model to C://Users//310304075//Desktop//ResnetOutputs//weights//resnetT1_001_8670047006359552.00.hdf5\n",
      "Epoch 2/3\n",
      "1/2 [==============>...............] - ETA: 11s - loss: 864031187402752.0000(4, 336, 336, 9)\n",
      "training\n",
      "(4, 336, 336, 3)\n",
      "(4, 336, 336, 9)\n",
      "training\n",
      "(4, 336, 336, 3)\n",
      "(2, 336, 336, 9)\n",
      "validation\n",
      "(2, 336, 336, 3)\n",
      "2/2 [==============================] - 26s 13s/step - loss: 864031187402752.0000 - val_loss: 432015593701376.0000\n",
      "\n",
      "Epoch 00002: val_loss improved from 8670047006359552.00000 to 432015593701376.00000, saving model to C://Users//310304075//Desktop//ResnetOutputs//weights//resnetT1_002_432015593701376.00.hdf5\n",
      "Epoch 3/3\n",
      "1/2 [==============>...............] - ETA: 10s - loss: 864031187402752.0000(4, 336, 336, 9)\n",
      "training\n",
      "(4, 336, 336, 3)\n",
      "(4, 336, 336, 9)\n",
      "training\n",
      "(4, 336, 336, 3)\n",
      "(2, 336, 336, 9)\n",
      "validation\n",
      "(2, 336, 336, 3)\n",
      "2/2 [==============================] - 20s 10s/step - loss: 864031187402752.0000 - val_loss: 432015593701376.0000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 432015593701376.00000\n",
      "val_loss\n",
      "loss\n"
     ]
    }
   ],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=32, dim=(32,32,32), n_channelsx=9,\n",
    "                 n_channelsy=9, shuffle=True, phase='training'):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channelsx = n_channelsx\n",
    "        self.n_channelsy = n_channelsy\n",
    "        self.shuffle = shuffle\n",
    "        self.phase = phase\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channelsx))\n",
    "        y = np.empty((self.batch_size, *self.dim, self.n_channelsy))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('C://Users//310304075//Desktop//DataResnet//' + self.phase + '//LR_' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i,] = np.load('C://Users//310304075//Desktop//DataResnet//' + self.phase + '//HR_' + ID + '.npy')\n",
    "    \n",
    "        print (np.shape(X))\n",
    "        print (self.phase)\n",
    "        print (np.shape(y))\n",
    "        return X, y    \n",
    "    \n",
    "def convolutional_block(X, f, filters, stage, block, s = 1):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block, resnet\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1 = filters \n",
    "    F2 = filters\n",
    "    F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = keras.initializers.glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = keras.initializers.glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = keras.initializers.glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    ##### SHORTCUT PATH #### (≈2 lines)\n",
    "    X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1',\n",
    "                        kernel_initializer = keras.initializers.glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def relu_bn(inputs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Batch normalizationa and RELU\n",
    "    \"\"\"\n",
    "    bn = BatchNormalization(axis=-1, center=True, scale=False)(inputs)\n",
    "    bn = Activation('relu')(bn)\n",
    "    return bn\n",
    "\n",
    "\n",
    "def residual_module(layer_in):\n",
    "    merge_input = layer_in\n",
    "    merge_input = Conv2D(2, (1,1), padding='same', activation='relu', input_shape=(patch_size, patch_size, numberTS), kernel_initializer='he_normal')(merge_input)\n",
    "    merge_input = relu_bn(merge_input)\n",
    "    \n",
    "    Res1 = convolutional_block (merge_input, 3, 4, 1, \"Res1\", 1)\n",
    "    Res2 = convolutional_block (Res1, 3, 8, 2, \"Res2\", 1)\n",
    "    Res3 = convolutional_block (Res2, 3, 16, 3, \"Res3\", 1)\n",
    "    Res4 = convolutional_block (Res3, 3, 32, 4, \"Res4\", 1)\n",
    "    Res5 = convolutional_block (Res4, 3, 8, 5, \"Res5\", 1)\n",
    "    Res6 = convolutional_block (Res5, 3, 2, 6, \"Res6\", 1)\n",
    "            \n",
    "    layer_out = Conv2D(3, (1,1), padding='same', activation='relu', kernel_initializer='he_normal')(Res6)\n",
    "    \n",
    "    return layer_out\n",
    "    \n",
    "patch_size = 336\n",
    "numberTS = 9\n",
    "batch_size = 4\n",
    "\n",
    "# define model input\n",
    "visible = Input(batch_shape=(batch_size, patch_size, patch_size, numberTS))\n",
    "# add vgg module\n",
    "layer = residual_module(visible)\n",
    "# create model\n",
    "model = Model(inputs=visible, outputs=layer)\n",
    "# summarize model\n",
    "#model.summary()\n",
    "\n",
    "model.compile(SGD(lr=0.01, momentum=0.95), loss = 'mse')\n",
    "\n",
    "# save the model and weights\n",
    "model_name = 'ResNetT1'\n",
    "outdir=\"C://Users//310304075//Desktop//ResnetOutputs//\"\n",
    "weights_filepath = os.path.join(outdir, \"weights//resnetT1_{epoch:03d}_{val_loss:.2f}.hdf5\")\n",
    "model_filepath = os.path.join(outdir, 'ResnetT1.json')\n",
    "\n",
    "model_json = model.to_json() # serialize model to JSON\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "\n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(log_dir = os.path.join(outdir, 'logs//ResNetT1//'))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "\n",
    "# train the model\n",
    "#train_steps = train_gen.n//train_gen.batch_size\n",
    "#val_steps = val_gen.n//val_gen.batch_size\n",
    "\n",
    "ID={}\n",
    "ID[\"train\"] = ['1', '2', '3', '4','5','6','7','8','9','10']\n",
    "ID[\"validation\"] = ['1', '2']\n",
    "\n",
    "# Parameters\n",
    "paramstr = {'dim': (336,336),\n",
    "          'batch_size': 4,\n",
    "          'n_channelsx': 9,\n",
    "          'n_channelsy': 3,\n",
    "          'shuffle': True,\n",
    "          'phase':\"training\"}\n",
    "\n",
    "paramsval = {'dim': (336,336),\n",
    "          'batch_size': 2,\n",
    "          'n_channelsx': 9,\n",
    "          'n_channelsy': 3,\n",
    "          'shuffle': False,\n",
    "          'phase':\"validation\"}\n",
    "\n",
    "# Datasets\n",
    "partition = ID\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], **paramstr)\n",
    "validation_generator = DataGenerator(partition['validation'], **paramsval)\n",
    "\n",
    "history = model.fit_generator(generator=training_generator, steps_per_epoch=2, \n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=3,\n",
    "                    callbacks=callbacks_list\n",
    "                    )\n",
    "\n",
    "for key in history.history:\n",
    "    print(key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
